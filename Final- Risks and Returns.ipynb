{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4aaad683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc6abbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load thedata into a Pandas DataFrame.\n",
    "data = pd.read_csv(\"HistoricalPrices-3.csv\")\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "data_normalized = scaler.fit_transform(data.iloc[:, 4].values.reshape(-1, 1))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data_normalized, test_data_normalized = train_test_split(data_normalized, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f0a843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the state function\n",
    "def get_state(state_index, data):\n",
    "    lookback_window = 5  # Number of previous time steps to consider\n",
    "    start_index = max(0, state_index - lookback_window)\n",
    "    end_index = state_index + 1\n",
    "    state = data[start_index:end_index].flatten()\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "921f1b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0456,  0.0349],\n",
       "        [ 0.0525, -0.0298],\n",
       "        [ 0.0288,  0.0558],\n",
       "        ...,\n",
       "        [-0.0135, -0.0361],\n",
       "        [-0.0093, -0.0067],\n",
       "        [-0.0085,  0.0380]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the actions\n",
    "num_actions = 2  # Buy (1) or Hold (0)\n",
    "action_mapping = {0: 'Hold', 1: 'Buy'}\n",
    "\n",
    "# Create a Q-table, which is a table that stores the Q-values for each state-action pair.\n",
    "num_train_states = train_data_normalized.shape[0]\n",
    "num_test_states = test_data_normalized.shape[0]\n",
    "q_table = torch.zeros((num_train_states, num_actions))\n",
    "# Initialize the Q-table with random values.\n",
    "nn.init.xavier_uniform_(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07925432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the epsilon-greedy policy\n",
    "def epsilon_greedy_policy(state, q_table, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        action = random.randint(0, num_actions - 1)\n",
    "    else:\n",
    "        action = torch.argmax(q_table[state])\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01c9daf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Bellman equation\n",
    "def bellman_equation(state, action, reward, next_state_index, q_table):\n",
    "    if next_state_index < num_train_states:\n",
    "        next_state = torch.tensor([next_state_index])\n",
    "        target = reward + gamma * torch.max(q_table[next_state])\n",
    "    else:\n",
    "        target = reward\n",
    "    q_table[state, action] = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e013996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training phase\n",
    "num_epochs = 30\n",
    "epsilon = 1.0\n",
    "gamma = 0.98\n",
    "replay_buffer = []\n",
    "batch_size = 32\n",
    "target_update_freq = 10\n",
    "target_q_table = q_table.clone()\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    for state in range(num_train_states - 1):\n",
    "        current_state = get_state(state, train_data_normalized)\n",
    "        action = epsilon_greedy_policy(state, q_table, epsilon)\n",
    "        reward = train_data_normalized[state + 1].item()  # Convert tensor to a Python scalar\n",
    "        next_state_index = state + 1\n",
    "        bellman_equation(state, action, reward, next_state_index, q_table)\n",
    "\n",
    "        # Store experience in replay buffer\n",
    "        replay_buffer.append((current_state, action, reward, next_state_index))\n",
    "\n",
    "        # Perform experience replay\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            batch = random.sample(replay_buffer, batch_size)\n",
    "            for batch_current_state, batch_action, batch_reward, batch_next_state_index in batch:\n",
    "                bellman_equation(batch_current_state, batch_action, batch_reward, batch_next_state_index, q_table)\n",
    "\n",
    "    # Update target Q-table\n",
    "    if i % target_update_freq == 0:\n",
    "        target_q_table = q_table.clone()\n",
    "\n",
    "    # Decay epsilon\n",
    "    epsilon *= 0.70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473ae3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing phase\n",
    "shares = 0\n",
    "portfolio = 10000\n",
    "profits = []\n",
    "returns = []\n",
    "risks = []\n",
    "for state in range(num_test_states - 1):\n",
    "    current_state = get_state(state, test_data_normalized)\n",
    "    action = torch.argmax(q_table[state])\n",
    "    if action == 1:\n",
    "        shares += 1\n",
    "    profit = scaler.inverse_transform([test_data_normalized[state + 1]]) * shares - portfolio\n",
    "    price_ratio = scaler.inverse_transform(test_data_normalized[state + 1].reshape(1, -1)) / scaler.inverse_transform(test_data_normalized[state].reshape(1, -1))\n",
    "    return_on_investment = (price_ratio * shares)\n",
    "    risk = ((return_on_investment - portfolio) / portfolio)  # Risk as a ratio of the initial portfolio\n",
    "    profits.append(profit)\n",
    "    returns.append(price_ratio)\n",
    "    risks.append(risk)\n",
    "    \n",
    "# Flatten the profits list\n",
    "flattened_profits = [item for sublist in profits for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1038fad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average return, total return, average risk, and total risk\n",
    "avg_return = float(sum(returns)) / len(returns)\n",
    "total_return = float(profits[-1])  # Total return is the final profit\n",
    "avg_risk = float(sum([abs(r) for r in risks])) / len(risks)  # Average of absolute risks\n",
    "total_risk = float(sum([abs(r) for r in risks]))  # Total risk is the sum of absolute risks\n",
    "\n",
    "# Print the calculated values\n",
    "print(f\"Average Return: {avg_return:.4f}\")\n",
    "print(f\"Total Return: {total_return:.4f}\")\n",
    "print(f\"Average Risk: {avg_risk:.4f}\")\n",
    "print(f\"Total Risk: {total_risk:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1c88f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "plt.plot(flattened_profits)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Profit')\n",
    "plt.title('Profits Gained over Time')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
